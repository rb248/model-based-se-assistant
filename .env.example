# .env.example - Copy to .env and fill in with your keys. Keep .env out of version control.

# LLM provider settings
LLM_PROVIDER=gemini
LLM_MODEL=gemini-pro
USE_GEMINI=true
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048

# Gemini (Google) API key - sign up at https://ai.google.dev/
GOOGLE_API_KEY=

# OpenAI fallback (optional but recommended for reliability)
OPENAI_API_KEY=

# LLM Retry and Fallback Configuration
# Max retries before falling back to secondary LLM
LLM_MAX_RETRIES=3
# Fallback provider when primary fails (openai for GPT-4o-mini)
LLM_FALLBACK_PROVIDER=openai
LLM_FALLBACK_MODEL=gpt-4o-mini

# Embeddings provider: google, ollama, huggingface
EMBEDDING_PROVIDER=google
EMBEDDING_MODEL=models/embedding-001

# Ollama (local fallback) - only if using Ollama locally
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# RAG backend: faiss or chroma
RAG_BACKEND=faiss
TOP_K_RETRIEVAL=3

# Observability
ENABLE_LANGSMITH=false
LANGSMITH_PROJECT=

# Debugging
DEBUG_MODE=false
LOG_LEVEL=INFO

# SANDBOX TIMEOUTS
SANDBOX_TIMEOUT=30

# Add any other environment variables you need below
